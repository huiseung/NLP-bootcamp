# NLP bootcamp - Transformer Is All You Need (5th, 19.10.12 ~ 19.12.28)
모두의연구소 flipped school 과정 중 하나인 NLP bootcamp에서 발표한 paper 목록과 그 자료들입니다.

* participant : 
* faciliator : 김보섭

## Schedule
### Week01 (19/10/12)
* Orientation
### Week02 (19/10/19)
* Attention Is All You Need
	+ Presenter : 이윤주  
	+ Paper : https://arxiv.org/abs/1706.03762
	+ Material : [Attention Is All You Need_이윤주.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week02/Attention%20Is%20All%20You%20Need_%EC%9D%B4%EC%9C%A4%EC%A3%BC.pdf)
* Universal Sentence Encoder
	+ Presenter : 염혜원
	+ Paper : https://arxiv.org/abs/1803.11175
	+ Material : [Universal Sentence Encoder_염혜원.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week02/Universal%20Sentence%20Encoder_%EC%97%BC%ED%98%9C%EC%9B%90.pdf)
### Week03 (19/10/26)
* Self-Attention with Relative Postion Representations
	+ Presenter : 김성운
	+ Paper : https://arxiv.org/abs/1803.02155
	+ Material : [Self-Attention with Relative Position Representation_김성운.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week03/Self-Attention%20with%20Relative%20Position%20Representation_%EA%B9%80%EC%84%B1%EC%9A%B4.pdf)
* Character-Level Language Modeling with Deeper Self-Attention
	+ Presenter : 양홍민
	+ Paper : https://arxiv.org/abs/1808.04444
	+ Material : [Character-Level Language Modeling with Deeper Self-Attention_양홍민.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week03/Character-Level%20Language%20Modeling%20with%20Deeper%20Self-Attention_%EC%96%91%ED%99%8D%EB%AF%BC.pdf)
### Week04 (19/11/02)
* Generating Wikipedia by Summarizing Long Sequences
	+ Presenter : 임한동
	+ Paper : https://arxiv.org/abs/1801.10198
	+ Material :[Generating Wikipedia by Summarizing Long Sequences_임한동.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week04/Generating%20Wikipedia%20by%20Summarizing%20Long%20Sequences_%EC%9E%84%ED%95%9C%EB%8F%99.pdf)
* Improving Language Understanding by Generative Pre-Training
  + Presenter : 임정섭
  + Paper : https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
  + Material : [Improving Language Understanding by Generative Pre-Training_임정섭.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week04/Improving%20Language%20Understanding%20by%20Generative%20Pre-Training_%EC%9E%84%EC%A0%95%EC%84%AD.pdf)
* Language Models are Unsupervised Multitask Learners (optional)
  * Paper : https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
### Week05 (19/11/09)
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
	+ Presenter : 박승일
	+ Paper : https://arxiv.org/abs/1810.04805
	+ Material : [BERT_ Pre-training of Deep Bidirectional Transformers for Language Understanding_박승일.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week05/BERT_%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding_%EB%B0%95%EC%8A%B9%EC%9D%BC.pdf)
* Multi-Task Deep Neural Networks for Natural Language Understanding
	+ Presenter : 백영상
	+ Paper : https://arxiv.org/abs/1901.11504
	+ Material : [Multi-Task Deep Neural Networks for Natural Language Understanding_백영상.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week05/Multi-Task%20Deep%20Neural%20Networks%20for%20Natural%20Language%20Understanding_%EB%B0%B1%EC%98%81%EC%83%81.pdf)
### Week06 (19/11/16)
* SpanBERT: Improving Pre-training by Representing and Predicting Spans
	+ Presenter : 김정미
	+ Paper : https://arxiv.org/abs/1907.10529
	+ Material : [SpanBERT: Improving Pre-training by Representing and Predicting Spans_김정미.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week06/SpanBERT:%20Improving%20Pre-training%20by%20Representing%20and%20Predicting%20Spans_%EA%B9%80%EC%A0%95%EB%AF%B8.pdf)
* ERNIE: Enhanced Representation through Knowledge Integration
  + Presenter : 양승무
  + Paper : https://arxiv.org/abs/1904.09223
  + Material : [ERNIE: Enhanced Representation through Knowledge Integration_양승무.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week06/ERNIE:%20Enhanced%20Representation%20through%20Knowledge%20Integration_%EC%96%91%EC%8A%B9%EB%AC%B4.pdf)
* Pre-Training with Whole Word Masking for Chinese BERT (optional)
  + Paper : https://arxiv.org/abs/1906.08101
### Week07 (19/11/23)
* RoBERTa: A Robustly Optimized BERT Pretraining Approach
	+ Presenter : 강성현
	+ Paper : https://arxiv.org/abs/1907.11692
	+ Material : [RoBERTa: A Robustly Optimized BERT Pretraining Approach_강성현.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week07/RoBERTa:%20A%20Robustly%20Optimized%20BERT%20Pretraining%20Approach_%EA%B0%95%EC%84%B1%ED%98%84.pdf)
* ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
	+ Presenter : 이기창
	+ Paper : https://arxiv.org/abs/1907.12412
	+ Reference : [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding_이기창.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week07/ERNIE%202.0:%20A%20Continual%20Pre-training%20Framework%20for%20Language%20Understanding_%EC%9D%B4%EA%B8%B0%EC%B0%BD.pdf)
### Week08 (19/11/30) 
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
	+ Presenter : 류지은
	+ Paper : https://arxiv.org/abs/1901.02860
	+ Material : [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context_류지은.pdf](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week08/Transformer-XL:%20Attentive%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context_%EB%A5%98%EC%A7%80%EC%9D%80.pdf)
* XLNet: Generalized Autoregressive Pretraining for Language Understanding
  + Presenter : 박성찬
  + Paper : https://arxiv.org/abs/1906.08237
  + Material : [XLNet: Generalized Autoregressive Pretraining for Language Understanding_박성찬.pdf ](https://github.com/modulabs/NLP-bootcamp/blob/master/5th/week08/XLNet:%20Generalized%20Autoregressive%20Pretraining%20for%20Language%20Understanding_%EB%B0%95%EC%84%B1%EC%B0%AC.pdf)
### Week09 (19/12/14)
* Cross-lingual Language Model Pretraining
  - Presenter : 한지윤
  - Paper : https://arxiv.org/abs/1901.07291
  - Material : 
* MASS: Masked Sequence to Sequence Pre-training for Language Generation
  - Presenter : 최병주
  - Paper : https://arxiv.org/abs/1905.02450
  - Material : 
### Week10 (19/12/21)
* Unified Language Model Pre-training for Natural Language Understanding and Generation
  - Presenter : 윤주성
  - Paper : https://arxiv.org/abs/1905.03197
  - Material :
* CTRL: A Conditional Transformer Language Model for Controllable Generation
	- Presenter : 조원호
	- Paper : https://arxiv.org/abs/1909.05858
	- Material : 
### Week11 (19/12/28)
* TinyBERT: Distilling BERT for Natural Language Understanding
  - Presenter : 허훈
  - Paper : https://arxiv.org/abs/1909.10351
  - Material :
* ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
  - Presenter : 김동완
  - Paper : https://arxiv.org/abs/1909.11942
  - Material :
* DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (optional)
  - Paper : https://arxiv.org/abs/1910.01108
